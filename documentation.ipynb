{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ffc4d59",
   "metadata": {},
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf504fa0",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset was generated using ChatGPT, following the format provided in the email. I have decided to include an additional column, which is the menu, for more context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ae12b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f312777",
   "metadata": {},
   "source": [
    "## General Message\n",
    "\n",
    "For this project, I decided to go with local models. I believe that cloud models can easily be implemented as the logic would still remain the same but will have superior performance compared to what I have.\n",
    "\n",
    "I made this project in my laptop with the following specs.\n",
    "\n",
    "Processor: i5 12500H\n",
    "\n",
    "RAM: 16GB\n",
    "\n",
    "GPU RTX3060 Laptop GPU\n",
    "\n",
    "VRAM: 6GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588d9c42",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba59763a",
   "metadata": {},
   "source": [
    "## External things needed to be installed\n",
    "\n",
    "This project will require Ollama, as Ollama is the software that I used to abstract the interaction with the models.\n",
    "\n",
    "[Ollama Download Page](https://ollama.com/download)\n",
    "\n",
    "For the LLM, I used `gemma3:1b-it-qat` as it is the most lightweight model from the Gemma 3 family.\n",
    "\n",
    "[Gemma 3 Ollama Page](https://ollama.com/library/gemma3)\n",
    "\n",
    "To install, run `ollama run gemma3:1b-it-qat` in CMD.\n",
    "\n",
    "For the embedder, I used `nomic-embed-text:v1.5`. It is the top embedder by download numbers in Ollama.\n",
    "\n",
    "[Nomic Embed Ollama Page](https://ollama.com/library/nomic-embed-text:v1.5)\n",
    "\n",
    "To install, run `ollama pull nomic-embed-text:v1.5` in CMD.\n",
    "\n",
    "Better models can be used if you can support them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408cb36d",
   "metadata": {},
   "source": [
    "## Python related things needed to be installed\n",
    "\n",
    "I have provided a `requirements.txt` file. To use, first create an environment for the project. The environment should then be activated. Once activated, the command `pip install -r requirements.txt` can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e187bff0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd2a3a1",
   "metadata": {},
   "source": [
    "## Setting up the server\n",
    "\n",
    "The main functionality is stored in the fast_api_side .py file. However, this cannot be run directly like a normal .py file. Instead, to call the server, the following command should be run in the CMD with the environment activated.\n",
    "\n",
    "`uvicorn fast_api_side:app --reload`\n",
    "\n",
    "Once the server is loaded, the API can now be interacted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d026f536",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf8c80",
   "metadata": {},
   "source": [
    "## Interacting with the API\n",
    "\n",
    "One way to interact with the API is through the docs page of Uvicorn. This can be accessed through\n",
    "\n",
    "`http://127.0.0.1:8000/docs`\n",
    "\n",
    "It should show two POST endpoints present. I have included a screenshot in the screenshot folder.\n",
    "\n",
    "Another way is through the user_side .py file I have included. The user_side .py file should only be ran if the Uvicorn server is up as it works by making a request to the server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f146a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4914bdd1",
   "metadata": {},
   "source": [
    "## user_side\n",
    "\n",
    "Below is the documentation for the user_side .py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c8126c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059964ba",
   "metadata": {},
   "source": [
    "This part of the file declares the base URL and the two endpoints. The first request is sending the user query to the submit endpoint.\n",
    "\n",
    "It is in this endpoint where the user query is used to find the best match in the database and a response is crafted by the LLM.\n",
    "\n",
    "```\n",
    "# API endpoint\n",
    "base_url = \"http://127.0.0.1:8000\"\n",
    "\n",
    "submit_query_url = f\"{base_url}/submit/\"\n",
    "booking_url = f\"{base_url}/book/\"\n",
    "\n",
    "user_input = input(\"Input: \")\n",
    "\n",
    "# Data to send\n",
    "payload = {\"query\": user_input}\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(submit_query_url, json=payload)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c366cc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef7a8a8",
   "metadata": {},
   "source": [
    "This part of the file handles the next step. The second request is sending the response of the user to the book endpoint.\n",
    "\n",
    "It is in this endpoint that the LLM will decide if the user wanted to book and then update the booking JSON file accordingly.\n",
    "\n",
    "```\n",
    "if response.status_code == 200:\n",
    "    submit_data = response.json()\n",
    "    print(\"Response:\", submit_data[\"response\"])\n",
    "    print(\"Found:\", submit_data[\"found\"])\n",
    "    print(\"Shop:\", submit_data[\"shop_tuple\"])\n",
    "\n",
    "    if submit_data[\"found\"]:\n",
    "        print(f\"Do you want to book a table\")\n",
    "\n",
    "        user_input = input(\"Input: \")\n",
    "\n",
    "        # Data to send\n",
    "        payload = {\n",
    "            \"user_input\": user_input,\n",
    "            \"shop_tuple\": submit_data[\"shop_tuple\"],\n",
    "        }\n",
    "\n",
    "        # Make the POST request\n",
    "        response = requests.post(booking_url, json=payload)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            booking_data = response.json()\n",
    "\n",
    "            print(\"Here is what we got:\")\n",
    "            print(\"Booking Status:\", booking_data[\"booked\"])\n",
    "            print(\"Details:\", booking_data[\"entry\"])\n",
    "\n",
    "        else:\n",
    "            print(\"Error:\", response.status_code, response.text)\n",
    "\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee975a2",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4d6ec4",
   "metadata": {},
   "source": [
    "## embed_data\n",
    "\n",
    "Below is the documentation for the embed_data .py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c79e6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8679945",
   "metadata": {},
   "source": [
    "This function chunks the data into their respective rows. This chunking is to ensure that the context will be limited to an entry of the dataset.\n",
    "\n",
    "```\n",
    "def line_chunk(data_df):\n",
    "    line_chunk_list = []\n",
    "    for index, row in data_df.iterrows():\n",
    "        name = row[\"name\"]\n",
    "        category = row[\"category\"]\n",
    "        location = row[\"location\"]\n",
    "        description = row[\"description\"]\n",
    "        menu = row[\"menu\"]\n",
    "\n",
    "        chunk = f\"Name: {name}\\nCategory: {category}\\nLocation: {location}\\nDescription: {description}\\nMenu: {menu}\"\n",
    "        print(chunk)\n",
    "\n",
    "        line_chunk_list.append(chunk)\n",
    "\n",
    "    return line_chunk_list\n",
    "```\n",
    "\n",
    "It returns a list of chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00403c8a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df459c",
   "metadata": {},
   "source": [
    "This function gets the embedding for each chunk using the RAG model for this project.\n",
    "\n",
    "```\n",
    "def get_embed(line_chunk_list, embed_model):\n",
    "    embedding_list = []\n",
    "    for chunk in line_chunk_list:\n",
    "        embed_response = ollama.embeddings(model=embed_model, prompt=chunk)\n",
    "        embedding_values = embed_response[\"embedding\"]\n",
    "        embedding_list.append(embedding_values)\n",
    "\n",
    "    return embedding_list\n",
    "```\n",
    "\n",
    "The function returns a list which contains the embedding values of each chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a65db4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73733417",
   "metadata": {},
   "source": [
    "This function stores the values from the embedding process so that embedding will not be performed everytime the API is called.\n",
    "\n",
    "```\n",
    "def store_embed_json(embedding_list, json_embed_path):\n",
    "    if json_embed_path.exists():\n",
    "        os.remove(json_embed_path)\n",
    "\n",
    "    with open(json_embed_path, \"w\") as embed_json:\n",
    "        json.dump(embedding_list, embed_json)\n",
    "```\n",
    "\n",
    "The values are stored in a JSON file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7673bb2f",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57c775e",
   "metadata": {},
   "source": [
    "## embed_retrieve\n",
    "\n",
    "Below is the documentation for the embed_retrieve .py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c77edf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098749ae",
   "metadata": {},
   "source": [
    "This code block loads the embedding list from the JSON file.\n",
    "\n",
    "```\n",
    "def load_embed_json(json_embed_path):\n",
    "    embedding_list = []\n",
    "\n",
    "    if json_embed_path.exists():\n",
    "        with open(json_embed_path, \"r\") as embed_json:\n",
    "            embedding_list = json.load(embed_json)\n",
    "\n",
    "    else:\n",
    "        print(\"Not Found\")\n",
    "\n",
    "    return embedding_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7382b9bc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbb143f",
   "metadata": {},
   "source": [
    "This code block performs cosine similarity. It takes in the embedded form of the query and compare it with all the entries in the embedding list.\n",
    "\n",
    "The list is then sorted so that the best match will be the first element in the list being returned by the function.\n",
    "\n",
    "```\n",
    "def cosine_similarity_sort(query_vector, embedding_list):\n",
    "\n",
    "    embed_cos_list = []\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for embedding_vector in embedding_list:\n",
    "\n",
    "        a = np.array(query_vector)\n",
    "        b = np.array(embedding_vector)\n",
    "\n",
    "        dot_product = np.dot(a, b)\n",
    "        norm_a = np.linalg.norm(a)\n",
    "        norm_b = np.linalg.norm(b)\n",
    "\n",
    "        cos_similar_value = dot_product / (norm_a * norm_b)\n",
    "\n",
    "        embed_cos_tuple = (cos_similar_value, count)\n",
    "\n",
    "        embed_cos_list.append(embed_cos_tuple)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # Sort to get the best result at the top\n",
    "    sorted_embed_cos_list = sorted(embed_cos_list, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    return sorted_embed_cos_list\n",
    "```\n",
    "\n",
    "I have opted to do it this way since the dataset is just small and the time spent comparing will not be that long.\n",
    "\n",
    "Cosine similarity works by comparing the directions of the two vectors being compared. Since they exist in a higher dimension, a sense of how close they are pointing can be done through their dot products. Vectors aligned perfectly will return a value of 1 and vectors perpendicular will return a value of 0. Directionality matters since this conveys the \"context\" of the chunk. Chunks pointing in the same direction are likely to have similar meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b708e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673d00f9",
   "metadata": {},
   "source": [
    "This code block returns the top 3 entries based on the result of cosine similarity.\n",
    "\n",
    "```\n",
    "def get_top_list(dataset_df, user_input, embedding_list):\n",
    "    # Embed user input\n",
    "    user_input_embedding_values = ollama.embeddings(\n",
    "        model=\"nomic-embed-text:v1.5\", prompt=user_input\n",
    "    )[\"embedding\"]\n",
    "\n",
    "    embed_cos_list = cosine_similarity_sort(user_input_embedding_values, embedding_list)\n",
    "\n",
    "    count = 0\n",
    "    max_count = 3\n",
    "\n",
    "    print(f\"Top {max_count} results based on similarity to query\")\n",
    "\n",
    "    top_list = []\n",
    "\n",
    "    while count < max_count:\n",
    "\n",
    "        current_row = dataset_df.iloc[embed_cos_list[count][1]]\n",
    "\n",
    "        cos_score = embed_cos_list[count][0]\n",
    "\n",
    "        name = current_row[\"name\"]\n",
    "        category = current_row[\"category\"]\n",
    "        location = current_row[\"location\"]\n",
    "        description = current_row[\"description\"]\n",
    "        menu = current_row[\"menu\"]\n",
    "\n",
    "        print(\n",
    "            f\"Name: {name}\\nCategory: {category}\\nLocation: {location}\\nDescription: {description}\\nMenu: {menu}\\nScore: {cos_score}\\n\"\n",
    "        )\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        if cos_score > 0.5:\n",
    "            top_list.append((name, category, location, description, menu))\n",
    "\n",
    "    return top_list\n",
    "```\n",
    "\n",
    "It only accepts an entry if the cosine score is greater than 0.5. The returned data can then be used as context when answering the query of the user. This context is provided to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bc26be",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6d068e",
   "metadata": {},
   "source": [
    "## augment_generate\n",
    "\n",
    "Below is the documentation for the augment_generate .py file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f30b4c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424228d5",
   "metadata": {},
   "source": [
    "This code block ensures a unique ID is created when logging.\n",
    "\n",
    "```\n",
    "def get_formatted_time():\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    random_id = f\"{random.randint(0, 99999999):08d}\"\n",
    "    return f\"{timestamp}_{random_id}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be81caa2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b41f39",
   "metadata": {},
   "source": [
    "This function takes in the top 3 matches from embedding, the user input, and the model that will be used.\n",
    "\n",
    "So I made the LLM have the system prompt of being an assistant. The function will check if the top list is empty or not. If it is empty, this means no match and the LLM should inform the user that there is no match.\n",
    "\n",
    "If there is a match, the LLM will then take the information from the top list and use that as a context to generate a response based on the input of the user.\n",
    "\n",
    "The code looks long but it is mainly composed of messages for the LLM. The logic of the function is simply check if top list is not empty then craft a response.\n",
    "\n",
    "```\n",
    "def generate_response(top_list, user_input, model_name):\n",
    "\n",
    "    message_list = []\n",
    "\n",
    "    # System prompt\n",
    "    system_prompt = \"\"\"\n",
    "        You are a Business Lookup Assistant. You will help the user look for business that closely aligns with their requests.\n",
    "    \"\"\"\n",
    "\n",
    "    # Appends the system prompt\n",
    "    message_list.append(\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Appends the user query\n",
    "    message_list.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"This is the user query: {user_input}.\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # If no match\n",
    "    if len(top_list) == 0:\n",
    "\n",
    "        message_list.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"After searching our database, there is no relevant result.\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        message_list.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Create a response informing the user that we did not find a good match in our database. Do not offer any other help or extra information.\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        message_list.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Just reply with the response of having no match.\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        message_list.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Also keep it simple.\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        shop_tuple = (\"No Match\", \"No Match\", \"No Match\")\n",
    "\n",
    "        result_found = False\n",
    "\n",
    "    # If match\n",
    "    else:\n",
    "\n",
    "        message_list.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"This is the most relevant result:\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        name = top_list[0][0]\n",
    "        category = top_list[0][1]\n",
    "        location = top_list[0][2]\n",
    "        description = top_list[0][3]\n",
    "        menu = top_list[0][4]\n",
    "\n",
    "        message_list.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Name: {name}\\nCategory: {category}\\nLocation: {location}\\nDescription: {description}\\nMenu: {menu}\\n\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        message_list.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Synthesize a response to answer the query based on the most relevant result. Make it engaging.\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        message_list.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Make sure to tell the name of the shop, the location, the type, the description, and the menu. Do not offer any other help or extra information.\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        message_list.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Also keep it simple.\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        shop_tuple = (name, category, location)\n",
    "\n",
    "        result_found = True\n",
    "\n",
    "    # Uses the specified model to generate response\n",
    "    response: ChatResponse = chat(\n",
    "        model=model_name,\n",
    "        messages=message_list,\n",
    "    )\n",
    "\n",
    "    response_message = response.message.content\n",
    "\n",
    "    return response_message, result_found, shop_tuple\n",
    "```\n",
    "\n",
    "So in the end, the function returns the LLM response, a boolean corresponding if a match is found, and a tuple containing the details of the match. This will be used for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aa0ab7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7b4705",
   "metadata": {},
   "source": [
    "This code block is for parsing the response of the user if a match is found. The user will be asked if they want to book a table. \n",
    "While I could use a simple yes or no, I decided to use an LLM to catch their response assuming that they will type their thoughts.\n",
    "Again this code block is long because of the messages that I appended as instructions for the LLM.\n",
    "\n",
    "For inputs, it takes in the response of the user, the info of the shop, and the LLM being used.\n",
    "\n",
    "\n",
    "```\n",
    "def book_table_command(user_input, shop_tuple, model_name):\n",
    "\n",
    "    message_list = []\n",
    "\n",
    "    # System prompt\n",
    "    system_prompt = f\"\"\"\n",
    "        You are a booking table assistant. You will check if the user expressed their desire to book a table at {shop_tuple[0]}.\n",
    "    \"\"\"\n",
    "\n",
    "    # Appends the system prompt\n",
    "    message_list.append(\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    message_list.append(\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Note that they have been asked already if they want to book a table at {shop_tuple[0]}.\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    message_list.append(\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"This is their response.\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    message_list.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_input,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    message_list.append(\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"If the user input is gibberish or not related to booking a table, NO must be the response.\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    message_list.append(\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Respond with either YES or NO only. No punctuations in the response. Either YES or NO only. I repeat, YES or NO only.\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response: ChatResponse = chat(\n",
    "        model=model_name,\n",
    "        messages=message_list,\n",
    "    )\n",
    "\n",
    "    response_message = response.message.content\n",
    "    print(response_message)\n",
    "\n",
    "    if response_message.lower().strip() == \"yes\":\n",
    "\n",
    "        user_id = get_formatted_time()\n",
    "        shop_name = shop_tuple[0]\n",
    "        shop_category = shop_tuple[1]\n",
    "        shop_location = shop_tuple[2]\n",
    "        user_book_message = user_input\n",
    "\n",
    "        booking_entry_dict = {\n",
    "            \"user_id\": user_id,\n",
    "            \"shop_name\": shop_name,\n",
    "            \"shop_category\": shop_category,\n",
    "            \"shop_location\": shop_location,\n",
    "            \"message\": user_book_message,\n",
    "        }\n",
    "\n",
    "        book_status = True\n",
    "\n",
    "    else:\n",
    "        booking_entry_dict = {}\n",
    "\n",
    "        book_status = False\n",
    "\n",
    "    return book_status, booking_entry_dict\n",
    "```\n",
    "\n",
    "At the end, it will just return a book_status, which is a boolean, and a dictionary containing the details of the booking.\n",
    "\n",
    "The details stored is the user_id, the name and details of the shop, and the message sent by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5ab7fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a4147d",
   "metadata": {},
   "source": [
    "This code block modifies the booking.json file to be updated once a booking is confirmed.\n",
    "\n",
    "```\n",
    "def modify_book_json(json_book_path, booking_entry_dict):\n",
    "\n",
    "    # Create the JSON file\n",
    "    if not json_book_path.exists():\n",
    "        with open(json_book_path, \"w\") as file:\n",
    "            json.dump([], file)\n",
    "\n",
    "    with open(json_book_path, \"r\") as file:\n",
    "        try:\n",
    "            booking_data = json.load(file)\n",
    "        except json.JSONDecodeError:\n",
    "            booking_data = []\n",
    "\n",
    "    booking_data.append(booking_entry_dict)\n",
    "\n",
    "    # Write new data\n",
    "    with open(json_book_path, \"w\") as file:\n",
    "        json.dump(booking_data, file, indent=4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7795585",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32d60fa",
   "metadata": {},
   "source": [
    "## fast_api_side\n",
    "\n",
    "Below is the documentation for the fast_api_side .py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e678ba57",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb38388",
   "metadata": {},
   "source": [
    "This loads the dataframe and the embedding list so that the API can be called without reloading them everytime.\n",
    "\n",
    "```\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    try:\n",
    "        df = pd.read_csv(\"expanded_dataset.csv\")\n",
    "        json_path = Path(\"embedding.json\")\n",
    "        embedding_list = embed_retrieve.load_embed_json(json_path)\n",
    "\n",
    "        # Store in app state\n",
    "        app.state.dataset = df\n",
    "        app.state.embeddings = embedding_list\n",
    "\n",
    "        print(\"Data loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "\n",
    "    yield\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8afcf3c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534758ae",
   "metadata": {},
   "source": [
    "This is the part that of the API that takes in the input. The input is sent to the submit endpoint, where the code that process the input with the LLM and appropriate context is performed.\n",
    "\n",
    "```\n",
    "class QueryInput(BaseModel):\n",
    "    query: str\n",
    "\n",
    "\n",
    "# Endpoint that handles user queries\n",
    "@app.post(\"/submit/\")\n",
    "async def submit_query(input_data: QueryInput, request: Request):\n",
    "    df = request.app.state.dataset\n",
    "    embedding_list = request.app.state.embeddings\n",
    "    llm_name = \"gemma3:1b-it-qat\"\n",
    "\n",
    "    user_input = input_data.query\n",
    "\n",
    "    top_list = embed_retrieve.get_top_list(df, user_input, embedding_list)\n",
    "    response_message, result_found, shop_tuple = augment_generate.generate_response(\n",
    "        top_list, user_input, llm_name\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"response\": response_message,\n",
    "        \"found\": result_found,\n",
    "        \"shop_tuple\": shop_tuple,\n",
    "    }\n",
    "```\n",
    "\n",
    "After processing, the API will return a dictionary containing the response, the status whether a match is found, and the appropriate shop details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb4535",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83183c2",
   "metadata": {},
   "source": [
    "This is the part of the API that handles booking. The reply of the user is sent back again to the LLM. The LLM decides whether the user has shown desire to book.\n",
    "\n",
    "If yes, the \"user_id\", name of the store, location of the store, and the message of the user is collected.\n",
    "\n",
    "```\n",
    "class BookingInput(BaseModel):\n",
    "    user_input: str\n",
    "    shop_tuple: tuple\n",
    "\n",
    "# Endpoint that handles booking\n",
    "@app.post(\"/book/\")\n",
    "async def book_table(input_data: BookingInput):\n",
    "    from pathlib import Path\n",
    "\n",
    "    json_book_path = Path(\"booking.json\")  # or whatever your booking file is\n",
    "    llm_name = \"gemma3:1b-it-qat\"\n",
    "\n",
    "    book_status, booking_entry_dict = augment_generate.book_table_command(\n",
    "        input_data.user_input, input_data.shop_tuple, llm_name\n",
    "    )\n",
    "\n",
    "    if book_status:\n",
    "        augment_generate.modify_book_json(json_book_path, booking_entry_dict)\n",
    "\n",
    "    return {\"booked\": book_status, \"entry\": booking_entry_dict if book_status else None}\n",
    "```\n",
    "\n",
    "The collected data can then be stored in a separate file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
